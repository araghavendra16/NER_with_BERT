NER stands for Named Entity Recognition, which is a natural language processing (NLP) technique used to identify and classify named entities mentioned in unstructured text into predefined categories such as person names, organizations, locations, dates, numerical expressions, etc.

BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art NLP model developed by Google. 
It utilizes a Transformer architecture to pre-train deep bidirectional representations of text, achieving impressive results on various NLP tasks such as question answering, sentiment analysis, and text classification.

In this notebook, we perform NER with BERT. We use SimpleTransformers. 
SimpleTransformers is a Python library built on top of the Transformers library by Hugging Face, designed to simplify the process of training and using Transformer-based models for various natural language processing (NLP) tasks. 
It provides a high-level API that abstracts away much of the complexity involved in fine-tuning Transformer models like BERT, GPT, RoBERTa, etc., for tasks such as text classification, question answering, named entity recognition, and more.
